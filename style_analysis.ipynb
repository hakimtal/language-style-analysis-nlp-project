{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNveu4C8-hMi"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Paste your token here (or use an environment variable for security)\n",
        "HUGGINGFACE_TOKEN = \"\"\n",
        "\n",
        "# Log in\n",
        "login(token=HUGGINGFACE_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtahmKYBcc8Z"
      },
      "outputs": [],
      "source": [
        "id = {\n",
        "    \"name\": \"id\",\n",
        "    \"question\": [\"question\"],\n",
        "    \"choices\": [\"choices\"],\n",
        "    \"answer\": [\"answer\"],\n",
        "    \"instruction\": [\"instruction: You are given a question and 4 possible answers (A, B, C, D), you should answer only one.\"]\n",
        "}\n",
        "children = {\n",
        "    \"name\": \"children\",\n",
        "    \"question\": [\n",
        "        \"Hereâ€™s the question!\",\n",
        "        \"Guess what? Hereâ€™s your question!\",\n",
        "        \"Hereâ€™s your brain teaser!\",\n",
        "        \"Okay, quiz time!\",\n",
        "        \"Ready? Hereâ€™s the puzzle!\",\n",
        "        \"Hereâ€™s a tricky one for you!\",\n",
        "        \"I have a question for you\",\n",
        "        \"Heyyy, hereâ€™s something to figure out!\",\n",
        "        \"Time for a fun question!\",\n",
        "        \"Letâ€™s play â€” hereâ€™s a question!\"\n",
        "    ],\n",
        "    \"answer\": [\n",
        "        \"Tell me your answer!\",\n",
        "        \"Tell me what you think!\",\n",
        "        \"Whatâ€™s your pick?\",\n",
        "        \"Give me your guess\",\n",
        "        \"Shout out your choice!\",\n",
        "        \"Tell me the right answer!\",\n",
        "        \"And what would you say?\",\n",
        "        \"Give me your guess!\",\n",
        "        \"Which one do you think it is?\",\n",
        "        \"Tell me whats true!\"\n",
        "    ],\n",
        "    \"choices\": [\n",
        "        \"Pick one of these\",\n",
        "        \"You can pick one of these\",\n",
        "        \"Hereâ€™s your answers\",\n",
        "        \"Here are your four picks!\",\n",
        "        \"Your choices are right here\",\n",
        "        \"Your options\",\n",
        "        \"Here are four magical answers\",\n",
        "        \"Pick from these fun choices\",\n",
        "        \"One of these is the winner!\",\n",
        "        \"Hereâ€™s your list of answers\"\n",
        "    ],\n",
        "    \"instruction\": [\n",
        "        \"Youâ€™ll see four choices (A, B, C, D). Choose the best one!\",\n",
        "        \"Look at the question and the four answers (A, B, C, D). Tell the right one!\",\n",
        "        \"I give you a question and four choices (A, B, C, D). Choose the one you think is the winner!\",\n",
        "        \"Look at the question and the four answers (A, B, C, D). Choose right\",\n",
        "        \"Youâ€™ve got four answers (A, B, C, D). Pick the best one!\",\n",
        "        \"Read the question, look at the four answers (A, B, C, D), and choose the winner!\",\n",
        "        \"Pick the most magical and correct one from (A, B, C, D) to finish the story!\",\n",
        "        \"Choose the answer that makes the most sense.\",\n",
        "        \"Find the right answer from the four choices.\",\n",
        "        \"Point to the answer you think is correct.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "academic = {\n",
        "    \"name\": \"academic\",\n",
        "    \"question\": [\n",
        "        \"This constitutes the inquiry.\",\n",
        "        \"The inquiry is as presented below.\",\n",
        "        \"This is the primary research question.\",\n",
        "        \"Here follows the problem statement.\",\n",
        "        \"The following constitutes the posed inquiry.\",\n",
        "        \"Presented here is the investigative prompt.\",\n",
        "        \"The central question is as follows.\",\n",
        "        \"This is the scholarly query under examination.\",\n",
        "        \"The matter for academic consideration is:\",\n",
        "        \"The problem to solve is presented thus.\"\n",
        "    ],\n",
        "    \"answer\": [\n",
        "        \"Provide the substantiated response.\",\n",
        "        \"Articulate the most accurate conclusion.\",\n",
        "        \"Document your chosen response.\",\n",
        "        \"Submit your formal answer.\",\n",
        "        \"Submit your considered response.\",\n",
        "        \"Record your definitive conclusion.\",\n",
        "        \"Offer the correct alternative.\",\n",
        "        \"State the academically sound choice.\",\n",
        "        \"Indicate the correct resolution.\",\n",
        "        \"Present your evidence-based selection.\"\n",
        "    ],\n",
        "    \"choices\": [\n",
        "        \"Enumerated options are provided.\",\n",
        "        \"Consider the enumerated alternatives.\",\n",
        "        \"The possible solutions are detailed herein.\",\n",
        "        \"Reference the provided answer set.\",\n",
        "        \"Refer to the enumerated alternatives.\",\n",
        "        \"A finite set of responses is provided.\",\n",
        "        \"The listed alternatives are given below.\",\n",
        "        \"Multiple potential answers are included.\",\n",
        "        \"The following four options are considered:\",\n",
        "        \"Your possible selections are outlined herein.\"\n",
        "    ],\n",
        "    \"instruction\": [\n",
        "        \"You are presented with a question accompanied by four alternatives (A, B, C, D). Select the most accurate alternative.\",\n",
        "        \"Review the prompt and four options (A, B, C, D). Select the one supported by the strongest evidence.\",\n",
        "        \"From the provided query and four candidate answers (A, B, C, D), determine the one most consistent with established theory.\",\n",
        "        \"You are to evaluate one question and four listed answers (A, B, C, D). Choose the answer of highest validity.\",\n",
        "        \"Analyze the question and evaluate four possible answers (A, B, C, D). Select the one with the highest empirical validity.\",\n",
        "        \"Determine the correct choice based on logical and factual alignment.\",\n",
        "        \"Examine all provided responses and select the one most consistent with empirical data.\",\n",
        "        \"Identify the answer that most accurately reflects established knowledge.\",\n",
        "        \"Apply critical reasoning to select the correct choice.\",\n",
        "        \"Choose the response with the greatest theoretical support.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "gen_alpha = {\n",
        "    \"name\": \"gen_alpha\",\n",
        "    \"question\": [\n",
        "        \"Hereâ€™s the Q, bestie.\",\n",
        "        \"Yo, hereâ€™s the Q.\",\n",
        "        \"Check this out, fam â€” hereâ€™s the question.\",\n",
        "        \"Whatâ€™s up? Hereâ€™s the Q.\",\n",
        "        \"Hey squad, Q time.\",\n",
        "        \"Look alive, hereâ€™s the question.\",\n",
        "        \"Ready for this Q?\",\n",
        "        \"Big brain time â€” hereâ€™s the Q.\",\n",
        "        \"This oneâ€™s for you, fam.\",\n",
        "        \"Question incoming!\"\n",
        "    ],\n",
        "    \"answer\": [\n",
        "        \"Drop your answer.\",\n",
        "        \"Shoot me your A.\",\n",
        "        \"Tell me whatâ€™s lit.\",\n",
        "        \"Send it, bestie.\",\n",
        "        \"Lock in your choice.\",\n",
        "        \"Hit me with your pick.\",\n",
        "        \"Give me the right one.\",\n",
        "        \"Flex your smarts.\",\n",
        "        \"Pick your fave answer.\",\n",
        "        \"Make it official â€” whatâ€™s your answer?\"\n",
        "    ],\n",
        "    \"choices\": [\n",
        "        \"Pick from these options.\",\n",
        "        \"Your picks are here.\",\n",
        "        \"Check these out.\",\n",
        "        \"Here are your choices, fr.\",\n",
        "        \"Lineupâ€™s right here.\",\n",
        "        \"Peep the options.\",\n",
        "        \"Scroll through these picks.\",\n",
        "        \"Your menu is here.\",\n",
        "        \"These are the contenders.\",\n",
        "        \"The squad of answers awaits.\"\n",
        "    ],\n",
        "    \"instruction\": [\n",
        "        \"Youâ€™ll get a question with four choices (A, B, C, D). Choose the most accurate one.\",\n",
        "        \"Read the question and pick the GOAT answer from (A, B, C, D).\",\n",
        "        \"Four options (A, B, C, D) are on the table. Pick the most valid one.\",\n",
        "        \"Only one of (A, B, C, D) is bussinâ€™ â€” find it.\",\n",
        "        \"Snag the best answer and flex your brain.\",\n",
        "        \"Lock in the choice that hits different.\",\n",
        "        \"Pick the option thatâ€™s giving main character energy.\",\n",
        "        \"No cap â€” choose the correct one.\",\n",
        "        \"Make your pick and own it.\",\n",
        "        \"Find the answer thatâ€™s 100% facts.\"\n",
        "    ]\n",
        "}\n",
        "gen_z = {\n",
        "    \"name\": \"gen_z\",\n",
        "    \"question\": [\n",
        "        \"Hereâ€™s the sitch ðŸ‘€\",\n",
        "        \"Whatâ€™s vibing?\",\n",
        "        \"Alright fam, hereâ€™s your Q.\",\n",
        "        \"So hereâ€™s the lowdown.\",\n",
        "        \"Hot take incoming â€” hereâ€™s the Q.\",\n",
        "        \"We got a question for you, no cap.\",\n",
        "        \"This oneâ€™s built diff â€” hereâ€™s the Q.\",\n",
        "        \"Lowkey important Q here.\",\n",
        "        \"You wonâ€™t guess this one.\"\n",
        "    ],\n",
        "    \"answer\": [\n",
        "        \"Drop the teaâ€¦ which oneâ€™s right?\",\n",
        "        \"Send your pick, fr fr.\",\n",
        "        \"Slide your answer here.\",\n",
        "        \"Spill it.\",\n",
        "        \"Bet you know this one â€” say it.\",\n",
        "        \"Type your answer, no ghosting.\",\n",
        "        \"Just hit me with the letter.\",\n",
        "        \"Make your call.\",\n",
        "        \"Donâ€™t overthink it â€” answer.\",\n",
        "        \"Your pick, fam?\"\n",
        "    ],\n",
        "    \"choices\": [\n",
        "        \"These are your picks, fam.\",\n",
        "        \"Options are here, bestie.\",\n",
        "        \"These are the options fr fr.\",\n",
        "        \"Hereâ€™s the lineup.\",\n",
        "        \"Check your menu of answers.\",\n",
        "        \"Peep the list.\",\n",
        "        \"Pick from these bangers.\",\n",
        "        \"The answer crew is here.\",\n",
        "        \"These are the contenders.\",\n",
        "        \"Hereâ€™s your squad of choices.\"\n",
        "    ],\n",
        "    \"instruction\": [\n",
        "        \"Four letters (A, B, C, D), one right choice â€” lock it in.\",\n",
        "        \"Youâ€™ve got four choices (A, B, C, D). Pick the one that hits right.\",\n",
        "        \"Read the Q, choose from (A, B, C, D). Simple.\",\n",
        "        \"Only one answer correct - find it\",\n",
        "        \"Pick the one thatâ€™s straight facts.\",\n",
        "        \"Select the answer's letter that wins.\",\n",
        "        \"No stress, just pick the best.\",\n",
        "        \"Trust your mind â€” choose the perfect answer.\",\n",
        "        \"Donâ€™t trip â€” itâ€™s just (A, B, C, or D).\",\n",
        "        \"Secure the bag with the right answer.\"\n",
        "    ]\n",
        "}\n",
        "millennial = {\n",
        "    \"name\": \"millennial\",\n",
        "    \"question\": [\n",
        "        \"Okay, hereâ€™s whatâ€™s up.\",\n",
        "        \"Hereâ€™s the question.\",\n",
        "        \"Your Q is right here.\",\n",
        "        \"Letâ€™s do this â€” hereâ€™s the Q.\",\n",
        "        \"Ready for your question?\",\n",
        "        \"This is the one weâ€™ve got for you.\",\n",
        "        \"Hereâ€™s your next challenge.\",\n",
        "        \"Hereâ€™s the prompt.\",\n",
        "        \"This is your question.\",\n",
        "        \"Pop quiz time!\"\n",
        "    ],\n",
        "    \"answer\": [\n",
        "        \"Send back your choice.\",\n",
        "        \"Reply with your answer.\",\n",
        "        \"Let me know your pick.\",\n",
        "        \"Drop your answer here.\",\n",
        "        \"Type your answer below.\",\n",
        "        \"Whatâ€™s your call?\",\n",
        "        \"Give me your selection.\",\n",
        "        \"Post your pick.\",\n",
        "        \"Make your choice.\",\n",
        "        \"Send in your answer.\"\n",
        "    ],\n",
        "    \"choices\": [\n",
        "        \"Hereâ€™s the lineup.\",\n",
        "        \"Options are here\",\n",
        "        \"These are your choices.\",\n",
        "        \"Your options are right here.\",\n",
        "        \"Check your picks.\",\n",
        "        \"Scroll through these answers.\",\n",
        "        \"This is your answer set.\",\n",
        "        \"Choices coming up.\",\n",
        "        \"The four answers are here.\",\n",
        "        \"Answer menu:\"\n",
        "    ],\n",
        "    \"instruction\": [\n",
        "        \"Question + 4 options (A, B, C, D). Pick the one thatâ€™s most accurate.\",\n",
        "        \"Read the question, scan the four choices (A, B, C, D), and choose the one that feels right.\",\n",
        "        \"Four possible answers (A, B, C, D). Choose the most accurate.\",\n",
        "        \"Youâ€™ve got a Q and four picks â€” choose wisely.\",\n",
        "        \"Consider the question and pick the most correct answer.\",\n",
        "        \"Pick the one youâ€™re confident in.\",\n",
        "        \"Make a choice between (A, B, C, D).\",\n",
        "        \"Read carefully and select the right answer.\",\n",
        "        \"Donâ€™t overthink â€” go with your first instinct.\",\n",
        "        \"Pick the one youâ€™d bet on.\"\n",
        "    ]\n",
        "}\n",
        "gen_x = {\n",
        "    \"name\": \"gen_x\",\n",
        "    \"question\": [\n",
        "        \"Hereâ€™s the deal -\",\n",
        "        \"Hereâ€™s the challenge: \",\n",
        "        \"This is the question: \",\n",
        "        \"Letâ€™s get to it â€” hereâ€™s your Q- \",\n",
        "        \"The question is as follows:\",\n",
        "        \"Hereâ€™s what you need to answer:\",\n",
        "        \"This is the one to solve- \",\n",
        "        \"Hereâ€™s the problem:\",\n",
        "        \"Hereâ€™s whatâ€™s on the table - \",\n",
        "        \"Question time:\"\n",
        "    ],\n",
        "    \"answer\": [\n",
        "        \"Whatâ€™s your call?\",\n",
        "        \"Call it.\",\n",
        "        \"Make your choice.\",\n",
        "        \"State your answer.\",\n",
        "        \"Give me your decision.\",\n",
        "        \"Your move.\",\n",
        "        \"Which one will it be?\",\n",
        "        \"Pick one.\",\n",
        "        \"Tell me the right one.\",\n",
        "        \"Choose and commit.\"\n",
        "    ],\n",
        "    \"choices\": [\n",
        "        \"These are your contenders.\",\n",
        "        \"Pick from these.\",\n",
        "        \"Here are the options.\",\n",
        "        \"Available picks below.\",\n",
        "        \"Choose from this list.\",\n",
        "        \"Your possible answers are here.\",\n",
        "        \"Hereâ€™s your answer set.\",\n",
        "        \"Select from these.\",\n",
        "        \"Options are below.\",\n",
        "        \"Here are the candidates.\"\n",
        "    ],\n",
        "    \"instruction\": [\n",
        "        \"One question, four options (A, B, C, D). Make your best pick.\",\n",
        "        \"Youâ€™ve got one question and four picks (A, B, C, D). Choose wisely.\",\n",
        "        \"Review the four choices (A, B, C, D) and pick the right one.\",\n",
        "        \"Only one is correct â€” choose it.\",\n",
        "        \"Pick the answer you know is right.\",\n",
        "        \"Go with the correct choice.\",\n",
        "        \"Select the letter of the correct answer.\",\n",
        "        \"Choose carefully.\",\n",
        "        \"Make your pick between (A, B, C, D).\",\n",
        "        \"Pick the one that fits best.\"\n",
        "    ]\n",
        "}\n",
        "boomer = {\n",
        "    \"name\": \"boomer\",\n",
        "    \"question\": [\n",
        "        \"Here is your question for review.\",\n",
        "        \"This is the question at hand.\",\n",
        "        \"The question is as follows.\",\n",
        "        \"Consider the following question.\",\n",
        "        \"Here is the prompt for you.\",\n",
        "        \"Presented for your consideration is this question.\",\n",
        "        \"This is the item to answer.\",\n",
        "        \"Hereâ€™s the matter to decide.\",\n",
        "        \"The next question is:\",\n",
        "        \"Please review this question.\"\n",
        "    ],\n",
        "    \"answer\": [\n",
        "        \"Please state your answer.\",\n",
        "        \"Provide your response.\",\n",
        "        \"Submit your answer.\",\n",
        "        \"Record your answer.\",\n",
        "        \"Select your response.\",\n",
        "        \"Mark your answer.\",\n",
        "        \"Indicate the correct choice.\",\n",
        "        \"Give your reply.\",\n",
        "        \"Write down your answer.\",\n",
        "        \"State your decision.\"\n",
        "    ],\n",
        "    \"choices\": [\n",
        "        \"Available responses:\",\n",
        "        \"These are the possible answers.\",\n",
        "        \"Your answer options are listed below.\",\n",
        "        \"Consider these possibilities.\",\n",
        "        \"Here are the four options.\",\n",
        "        \"Select from the following.\",\n",
        "        \"The possible answers are shown below.\",\n",
        "        \"These are the candidates.\",\n",
        "        \"You may choose from these.\",\n",
        "        \"The following are your answer options.\"\n",
        "    ],\n",
        "    \"instruction\": [\n",
        "        \"Evaluate the question and four responses (A, B, C, D). Select the correct one.\",\n",
        "        \"You are given a question with four options (A, B, C, D). Choose the correct one.\",\n",
        "        \"One question, four answers â€” pick the right one.\",\n",
        "        \"Read carefully and choose from (A, B, C, D).\",\n",
        "        \"Review and select the answer you believe is correct.\",\n",
        "        \"Pick the letter of the correct answer.\",\n",
        "        \"Make your selection based on the question.\",\n",
        "        \"From the four choices, choose the correct one.\",\n",
        "        \"Select your answer after reviewing all options.\",\n",
        "        \"Decide on the correct option.\"\n",
        "    ]\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxCFXb8vdN31"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "\n",
        "all_dicts = [id, academic, children, gen_alpha, gen_z, millennial, gen_x, boomer]\n",
        "\n",
        "def eval_model_all_dicts(model, task, dicts, output_dir=\"results\"):\n",
        "    results = {}\n",
        "    for d in tqdm(dicts, desc=\"Evaluating dictionaries\"):\n",
        "        # Evaluate the model with this dictionary\n",
        "        result = model.evaluate_task(task, d, output_dir=output_dir)\n",
        "        # Store the result with the dictionary name\n",
        "        results[d[\"name\"]] = result\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FztggPlfgSg"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "sampled_questions = json.load(open(\"data/general_test.json\"))\n",
        "uni_math_mcqs = json.load(open(\"data/difficulty_test/college_math_test.json\"))\n",
        "hs_math_mcqs = json.load(open(\"data/difficulty_test/hs_math_test.json\"))\n",
        "elem_math_mcqs = json.load(open(\"data/difficulty_test/elementary_math_test.json\"))\n",
        "history_mcqs = json.load(open(\"data/subjects_compare/history_test.json\"))\n",
        "cs_mcqs = json.load(open(\"data/subjects_compare/cs_test.json\"))\n",
        "biology_mcqs = json.load(open(\"data/subjects_compare/biology_test.json\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItWshgEfAyY2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from typing import List, Dict, Tuple\n",
        "import re\n",
        "from abc import ABC, abstractmethod\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "import random\n",
        "import json\n",
        "\n",
        "class MCQ(ABC):\n",
        "    def __init__(self, model=None, model_name: str = \"meta-llama/Llama-2-7b-hf\"):\n",
        "        \"\"\"\n",
        "        Initialize the MCQ handler with shared functionality for prompt formatting and choice extraction.\n",
        "\n",
        "        Args:\n",
        "            model: Preloaded local model (optional)\n",
        "            model_name: HuggingFace model name for local models\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def format_prompt(self, question_data: Dict, phrase_replacement_dict: Dict, output_field: str = None) -> str:\n",
        "        \"\"\"Format a prompt using the task's structured prompt format\"\"\"\n",
        "        choices = \"\"\n",
        "        for choice, answer in question_data['answers'].items():\n",
        "            choices += f\"({choice}) {answer}\\n\"\n",
        "        formatted = f\"{phrase_replacement_dict['instruction']}\\n{phrase_replacement_dict['question']}: {question_data['question']}\\n{phrase_replacement_dict['choices']}: {choices} \\n{phrase_replacement_dict['answer']}:\"\n",
        "        return formatted\n",
        "\n",
        "    @abstractmethod\n",
        "    def answer_mcq(self, question_data: Dict, phrase_replacement_dict) -> Dict:\n",
        "        \"\"\"Abstract method for answering a multiple choice question\"\"\"\n",
        "        pass\n",
        "\n",
        "    def evaluate_task(self, task_data: List[Dict], phrase_replacement_dict, output_dir: str = \"results\", save_results: bool = True, num_samples: int = None) -> Dict:\n",
        "        \"\"\"Evaluate the multiple-choice task and return detailed metrics.\"\"\"\n",
        "\n",
        "        # If num_samples is provided, randomly sample `num_samples` questions\n",
        "        all_questions = task_data\n",
        "        if num_samples:\n",
        "            all_questions = random.sample(task_data, min(num_samples, len(task_data)))\n",
        "\n",
        "        metrics = {\n",
        "            \"total_questions\": len(all_questions),\n",
        "            \"correct\": 0,\n",
        "            \"wrong\": 0,\n",
        "            \"per_choice_stats\": defaultdict(lambda: {\"correct\": 0, \"total\": 0}),\n",
        "            \"confidence_when_correct\": [],\n",
        "            \"confidence_when_wrong\": [],\n",
        "            \"all_results\": [],\n",
        "            \"model_name\": self.model_name\n",
        "        }\n",
        "\n",
        "        # Wrap the question processing with tqdm for progress indication\n",
        "        for q in tqdm(all_questions, desc=\"Evaluating\", unit=\"question\"):\n",
        "            tmp_dict = {k: random.choice(v) for k, v in phrase_replacement_dict.items()}\n",
        "            result = self.answer_mcq(q, tmp_dict)\n",
        "            metrics[\"all_results\"].append(result)\n",
        "\n",
        "            correct = result[\"answer\"] == result[\"correct_answer\"]\n",
        "            if correct:\n",
        "                metrics[\"correct\"] += 1\n",
        "                metrics[\"confidence_when_correct\"].append(result[\"confidence\"])\n",
        "            else:\n",
        "                metrics[\"wrong\"] += 1\n",
        "                metrics[\"confidence_when_wrong\"].append(result[\"confidence\"])\n",
        "\n",
        "            # Per-choice stats\n",
        "            true = result[\"correct_answer\"]\n",
        "            if true:\n",
        "                metrics[\"per_choice_stats\"][true][\"total\"] += 1\n",
        "                if correct:\n",
        "                    metrics[\"per_choice_stats\"][true][\"correct\"] += 1\n",
        "\n",
        "        metrics[\"accuracy\"] = metrics[\"correct\"] / metrics[\"total_questions\"]\n",
        "        metrics[\"per_choice_accuracy\"] = {\n",
        "            k: {\"accuracy\": v[\"correct\"] / v[\"total\"], \"total\": v[\"total\"]} for k, v in metrics[\"per_choice_stats\"].items()\n",
        "        }\n",
        "\n",
        "        # Save results\n",
        "        if save_results:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            fname = f\"results_task_{self.model_name}_{phrase_replacement_dict['name']}_{timestamp}.json\"\n",
        "            fname = fname.replace(\"/\", \"_\")\n",
        "            path = f\"{output_dir}/{fname}\"\n",
        "\n",
        "            # Convert defaultdict to regular dict for JSON\n",
        "            metrics[\"per_choice_stats\"] = dict(metrics[\"per_choice_stats\"])\n",
        "\n",
        "            with open(path, \"w\") as f:\n",
        "                json.dump(metrics, f, indent=2)\n",
        "\n",
        "        return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k95VdFvklU-L"
      },
      "source": [
        "# Llama 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbUfwajyoqdw"
      },
      "source": [
        "### install bits and bytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2t9eY1HGonic"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X21jXkXQovKa"
      },
      "source": [
        "### llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVCSj-mjDXIN"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "from typing import Dict\n",
        "\n",
        "class Llama2Model(MCQ):\n",
        "    def __init__(self, model_name: str = \"meta-llama/Llama-2-7b-hf\"):\n",
        "        super().__init__(model_name=model_name)\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "\n",
        "        # Initialize tokenizer and model\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\"\n",
        "        ).to(self.device)  # Move the model to the selected device\n",
        "\n",
        "    def answer_mcq(self, question_data: Dict, phrase_replacement_dict) -> Dict:\n",
        "        \"\"\"Answer a single multiple-choice question using Llama Model\"\"\"\n",
        "        premise = question_data[\"question\"]\n",
        "        choices = question_data[\"answers\"]  # Now we use 'answers' as the choices dictionary\n",
        "\n",
        "        # Format the prompt with choices\n",
        "        prompt = self.format_prompt(question_data, phrase_replacement_dict)\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)  # Ensure inputs are on the correct device\n",
        "        outputs = self.model(**inputs)\n",
        "\n",
        "        # Get logits for the last token in the sequence\n",
        "        logits = outputs.logits[:, -1, :]  # Get logits for the last token\n",
        "        probabilities = torch.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
        "\n",
        "        # Map each choice to its token ID and calculate probabilities\n",
        "        choice_probabilities = {}\n",
        "        for choice, answer in choices.items():\n",
        "            # Encode the choice text to get its token ID\n",
        "            choice_token_id = self.tokenizer.encode(answer, add_special_tokens=False)[0]  # Token ID for the choice text\n",
        "            # Get the probability of the choice token in the model's output\n",
        "            choice_probabilities[choice] = probabilities[0, choice_token_id].item()\n",
        "\n",
        "        # Select the choice with the highest probability\n",
        "        predicted_choice = max(choice_probabilities, key=choice_probabilities.get)\n",
        "        confidence = choice_probabilities[predicted_choice]\n",
        "\n",
        "        return {\n",
        "            \"question\": premise,\n",
        "            \"answers\": choices,\n",
        "            \"answer\": predicted_choice,\n",
        "            \"confidence\": confidence,\n",
        "            \"correct_answer\": question_data[\"correct_answer\"],  # No need to access the first character anymore\n",
        "            \"prompt_used\": prompt\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJqxutD8lZVj"
      },
      "outputs": [],
      "source": [
        "llama2_model = Llama2Model()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model_all_dicts(llama2_model, sampled_questions, all_dicts)"
      ],
      "metadata": {
        "id": "Zi05yqKsW46B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model_all_dicts(llama2_model, uni_math_mcqs, all_dicts, output_dir=\"results_by_subject/math/college\")\n",
        "eval_model_all_dicts(llama2_model, hs_math_mcqs, all_dicts, output_dir=\"results_by_subject/math/hs\")\n",
        "eval_model_all_dicts(llama2_model, elem_math_mcqs, all_dicts, output_dir=\"results_by_subject/math/elementary\")\n",
        "eval_model_all_dicts(llama2_model, history_mcqs, all_dicts, output_dir=\"results_by_subject/history\")\n",
        "eval_model_all_dicts(llama2_model, biology_mcqs, all_dicts, output_dir=\"results_by_subject/biology\")\n",
        "eval_model_all_dicts(llama2_model, cs_mcqs, all_dicts, output_dir=\"results_by_subject/cs\")"
      ],
      "metadata": {
        "id": "HYCrhv5akEiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIr6m7noDPAL"
      },
      "source": [
        "# Llama 3.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPWChRoZDOOD"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "class Llama3BModel(MCQ):\n",
        "    def __init__(self, model_name: str = \"meta-llama/Llama-3.2-3B-Instruct\"):\n",
        "        super().__init__(model_name=model_name)\n",
        "\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.tokenizer.padding_side = 'left'\n",
        "\n",
        "        # Load the Llama model\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            torch_dtype=torch.bfloat16,  # Use bfloat16 for efficiency\n",
        "            device_map=\"auto\",           # Automatically map model layers to available devices\n",
        "            trust_remote_code=True       # Trust custom code if available in the model repo\n",
        "        ).to(self.device)  # Move the model to the correct device\n",
        "\n",
        "        # Set up the text generation pipeline\n",
        "        self.llm = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            pad_token_id=self.tokenizer.eos_token_id,\n",
        "            max_new_tokens=1,  # Generate a single token for efficiency\n",
        "            do_sample=False     # For deterministic output\n",
        "        )\n",
        "\n",
        "    def answer_mcq(self, question_data: Dict, phrase_replacement_dict) -> Dict:\n",
        "        \"\"\"Answer a single multiple-choice question using Llama Model\"\"\"\n",
        "        choices = question_data[\"answers\"]  # Now we use 'answers' as the choices dictionary\n",
        "\n",
        "        # Format the prompt with choices\n",
        "        prompt = self.format_prompt(question_data, phrase_replacement_dict)\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)  # Ensure inputs are on the correct device\n",
        "\n",
        "        # Get model outputs (logits)\n",
        "        outputs = self.model(**inputs)\n",
        "        logits = outputs.logits[:, -1, :]  # Get logits for the last token in the sequence\n",
        "\n",
        "        # Apply softmax to get probabilities for each choice\n",
        "        probabilities = torch.softmax(logits, dim=-1)\n",
        "\n",
        "        # Map each choice to its token ID and calculate probabilities\n",
        "        choice_probabilities = {}\n",
        "        for idx, choice in enumerate(choices):\n",
        "            choice_token_id = self.tokenizer.encode(choice, add_special_tokens=False)[0]  # Token ID for the choice text\n",
        "            choice_probabilities[choice] = probabilities[0, choice_token_id].item()\n",
        "\n",
        "        # Select the choice with the highest probability\n",
        "        predicted_choice = max(choice_probabilities, key=choice_probabilities.get)\n",
        "        confidence = choice_probabilities[predicted_choice]\n",
        "\n",
        "        return {\n",
        "            \"question\": question_data[\"question\"],\n",
        "            \"choices\": choices,\n",
        "            \"answer\": predicted_choice,\n",
        "            \"confidence\": confidence,\n",
        "            \"correct_answer\": question_data[\"correct_answer\"],  # Assuming output field contains the correct answer\n",
        "            \"prompt_used\": prompt\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qs8kqHfzD5mk"
      },
      "outputs": [],
      "source": [
        "llama_3 = Llama3BModel()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model_all_dicts(llama_3, sampled_questions, all_dicts)"
      ],
      "metadata": {
        "id": "eqqZUZIpXe3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model_all_dicts(llama_3, uni_math_mcqs, all_dicts, output_dir=\"results_by_subject/math/college\")\n",
        "eval_model_all_dicts(llama_3, hs_math_mcqs, all_dicts, output_dir=\"results_by_subject/math/hs\")\n",
        "eval_model_all_dicts(llama_3, elem_math_mcqs, all_dicts, output_dir=\"results_by_subject/math/elementary\")"
      ],
      "metadata": {
        "id": "11WM89joRyr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model_all_dicts(llama_3, history_mcqs, all_dicts, output_dir=\"results_by_subject/history\")\n",
        "eval_model_all_dicts(llama_3, biology_mcqs, all_dicts, output_dir=\"results_by_subject/biology\")\n",
        "eval_model_all_dicts(llama_3, cs_mcqs, all_dicts, output_dir=\"results_by_subject/cs\")"
      ],
      "metadata": {
        "id": "9iwu0W-CWUSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2GkSNAAk-pO"
      },
      "source": [
        "# Bart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGOYk1HLDc_X"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "from transformers.models.bart.modeling_bart import BartForSequenceClassification # Explicitly import the model class\n",
        "\n",
        "\n",
        "# Define ENTAILMENT_IDX based on the task (usually 2 for MNLI)\n",
        "ENTAILMENT_IDX = 2\n",
        "\n",
        "class BartModel(MCQ):\n",
        "    def __init__(self, model_name: str = \"facebook/bart-large-mnli\"):\n",
        "        super().__init__(model_name=model_name)\n",
        "        # Load the tokenizer and model for BART from the 'facebook/bart-large-mnli' checkpoint\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def answer_mcq(self, question_data: dict, phrase_replacement_dict: dict) -> dict:\n",
        "        \"\"\"\n",
        "        Answer a single multiple-choice question efficiently using a BART-style model.\n",
        "        Choices are given as a dictionary (e.g., {\"A\": \"Choice1\", \"B\": \"Choice2\", ...})\n",
        "        \"\"\"\n",
        "\n",
        "        premise = question_data[\"question\"]\n",
        "        choices_dict = question_data[\"answers\"]  # e.g., {\"A\": \"Louis XIII\", ...}\n",
        "        choice_keys = list(choices_dict.keys())\n",
        "        choice_texts = list(choices_dict.values())\n",
        "\n",
        "        # Format the prompt with the phrase replacement dictionary\n",
        "        prompt = self.format_prompt(question_data, phrase_replacement_dict)\n",
        "\n",
        "        # Prepare the batched input: one sequence per choice\n",
        "        # Each input is: \"Prompt + Question + 'The answer is {choice_text}.'\"\n",
        "        batch_texts = [f\"{prompt}\\n\\nThe answer is {text}.\" for text in choice_texts]\n",
        "\n",
        "        # Tokenize as a batch\n",
        "        inputs = self.tokenizer(\n",
        "            batch_texts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True\n",
        "        ).to(self.model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(**inputs).logits\n",
        "            # Softmax over the last dimension and pick entailment index\n",
        "            probs = torch.softmax(logits, dim=1)[:, ENTAILMENT_IDX]\n",
        "\n",
        "        probs = probs.cpu().tolist()\n",
        "\n",
        "        # Pick the choice with the highest probability\n",
        "        max_idx = int(torch.tensor(probs).argmax())\n",
        "        predicted_key = choice_keys[max_idx]\n",
        "        predicted_answer = choice_texts[max_idx]\n",
        "        confidence = probs[max_idx]\n",
        "\n",
        "        return {\n",
        "            \"question\": premise,\n",
        "            \"choices\": choices_dict,\n",
        "            \"answer\": predicted_key,\n",
        "            \"confidence\": confidence,\n",
        "            \"correct_answer\": question_data.get(\"correct_answer\", None),\n",
        "            \"prompt_used\": prompt\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_KYYPAmGIHB"
      },
      "outputs": [],
      "source": [
        "bart = BartModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dpm5HO6hxQYR"
      },
      "outputs": [],
      "source": [
        "eval_model_all_dicts(bart, sampled_questions, all_dicts)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model_all_dicts(bart, uni_math_mcqs, all_dicts, output_dir=\"results_by_subject/math/college\")\n",
        "eval_model_all_dicts(bart, hs_math_mcqs, all_dicts, output_dir=\"results_by_subject/math/hs\")\n",
        "eval_model_all_dicts(bart, elem_math_mcqs, all_dicts, output_dir=\"results_by_subject/math/elementary\")\n",
        "eval_model_all_dicts(bart, history_mcqs, all_dicts, output_dir=\"results_by_subject/history\")\n",
        "eval_model_all_dicts(bart, biology_mcqs, all_dicts, output_dir=\"results_by_subject/biology\")\n",
        "eval_model_all_dicts(bart, cs_mcqs, all_dicts, output_dir=\"results_by_subject/cs\")"
      ],
      "metadata": {
        "id": "lwzWFkoXXtRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# deBERTa"
      ],
      "metadata": {
        "id": "hH_6LeofKoIB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PPSVG4iNbst"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, Any, List, Optional\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "class DeBERTaNLI_MCQ(MCQ):\n",
        "    \"\"\"\n",
        "    Multiple-choice via zero-shot NLI with:\n",
        "      model = MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\n",
        "      premise  = format_prompt(question_data, phrase_replacement_dict)\n",
        "      hypothesis(choice) = \"The answer is {choice}.\"\n",
        "      score = P(entailment | premise, hypothesis)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str =\n",
        "                 \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\"):\n",
        "        super().__init__(model_name=model_name)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name).to(self.device)\n",
        "        self.model.eval()\n",
        "        self.entail_idx = self._entailment_index()\n",
        "\n",
        "    def _entailment_index(self) -> int:\n",
        "        # Robustly detect the entailment label index from the model config\n",
        "        id2label = getattr(self.model.config, \"id2label\", None)\n",
        "        if isinstance(id2label, dict):\n",
        "            for i, name in id2label.items():\n",
        "                if str(name).lower().startswith(\"entail\"):\n",
        "                    return int(i)\n",
        "        # Fallback to common MNLI ordering (C, N, E) -> index 2\n",
        "        return 2\n",
        "\n",
        "    def answer_mcq(self, question_data: Dict[str, Any],\n",
        "                   phrase_replacement_dict: Dict[str, str]) -> Dict[str, Any]:\n",
        "        # Build premise with YOUR formatter (required as per your request)\n",
        "        premise = self.format_prompt(question_data, phrase_replacement_dict)\n",
        "\n",
        "        choices: Dict[str, str] = question_data[\"answers\"]  # {\"A\": \"...\", \"B\": \"...\"}\n",
        "        keys: List[str] = list(choices.keys())\n",
        "        texts: List[str] = [choices[k] for k in keys]\n",
        "\n",
        "        premises = [premise] * len(texts)\n",
        "        hyps = [f\"The answer is {t}.\" for t in texts]\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            premises,\n",
        "            hyps,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True\n",
        "        ).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(**inputs).logits  # [batch, 3]\n",
        "            probs = F.softmax(logits, dim=1)[:, self.entail_idx]  # entailment probs\n",
        "\n",
        "        best_idx = int(probs.argmax().item())\n",
        "        confidences = probs.detach().cpu().tolist()\n",
        "\n",
        "        return {\n",
        "            \"question\": question_data[\"question\"],\n",
        "            \"choices\": choices,\n",
        "            \"answer\": keys[best_idx],\n",
        "            \"answer_text\": texts[best_idx],\n",
        "            \"confidence\": confidences[best_idx],\n",
        "            \"all_entailment_probs\": {k: p for k, p in zip(keys, confidences)},\n",
        "            \"correct_answer\": question_data.get(\"correct_answer\"),\n",
        "            \"prompt_used\": premise,\n",
        "            \"model_name\": self.model.config._name_or_path\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deberta = DeBERTaNLI_MCQ()"
      ],
      "metadata": {
        "id": "3x8qMuCZKkT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model_all_dicts(deberta, sampled_questions, all_dicts)"
      ],
      "metadata": {
        "id": "B8JcBR_mX1z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model_all_dicts(deberta, uni_math_mcqs, all_dicts, output_dir=\"results_by_subject/math/college\")\n",
        "eval_model_all_dicts(deberta, hs_math_mcqs, all_dicts, output_dir=\"results_by_subject/math/hs\")\n",
        "eval_model_all_dicts(deberta, elem_math_mcqs, all_dicts, output_dir=\"results_by_subject/math/elementary\")\n",
        "eval_model_all_dicts(deberta, history_mcqs, all_dicts, output_dir=\"results_by_subject/history\")\n",
        "eval_model_all_dicts(deberta, biology_mcqs, all_dicts, output_dir=\"results_by_subject/biology\")\n",
        "eval_model_all_dicts(deberta, cs_mcqs, all_dicts, output_dir=\"results_by_subject/cs\")"
      ],
      "metadata": {
        "id": "8u2c_58UeyFv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "uXIxtVOiltaS",
        "LbUfwajyoqdw",
        "m2GkSNAAk-pO"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}