{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNveu4C8-hMi"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Paste your token here (or use an environment variable for security)\n",
        "HUGGINGFACE_TOKEN = \"\"\n",
        "\n",
        "# Log in\n",
        "login(token=HUGGINGFACE_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "# Allow letters, numbers, dashes, and underscores\n",
        "STYLE_RE = re.compile(r'_(?P<style>[A-Za-z0-9_-]+)_(?:\\d{8})_\\d{6}\\.json$')\n",
        "\n",
        "def extract_style(filename: str):\n",
        "    m = STYLE_RE.search(filename)\n",
        "    return m.group('style') if m else None"
      ],
      "metadata": {
        "id": "DIDUHDoMzucY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def evaluate_model(eval_model, prompt_dir):\n",
        "\n",
        "    folder = Path(f\"rephrased_prompts/{prompt_dir}\")\n",
        "    for p in folder.glob(\"rephrased_plain_one_option*.json\"):\n",
        "        style = extract_style(p.name)\n",
        "        with open(p, \"r\", encoding=\"utf-8\") as f:\n",
        "            file_data = json.load(f)\n",
        "\n",
        "        eval_model.evaluate_task(file_data, style)"
      ],
      "metadata": {
        "id": "YVKKBLtCrGE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItWshgEfAyY2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from typing import List, Dict, Tuple\n",
        "import re\n",
        "from abc import ABC, abstractmethod\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "import random\n",
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "class MCQ(ABC):\n",
        "    def __init__(self, model=None, model_name: str = \"meta-llama/Llama-2-7b-hf\"):\n",
        "        \"\"\"\n",
        "        Initialize the MCQ handler with shared functionality for prompt formatting and choice extraction.\n",
        "\n",
        "        Args:\n",
        "            model: Preloaded local model (optional)\n",
        "            model_name: HuggingFace model name for local models\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def format_prompt(self, question_data: Dict, phrase_replacement_dict: Dict, output_field: str = None) -> str:\n",
        "        \"\"\"Format a prompt using the task's structured prompt format\"\"\"\n",
        "        choices = \"\"\n",
        "        for choice, answer in question_data['answers'].items():\n",
        "            choices += f\"({choice}) {answer}\\n\"\n",
        "        formatted = f\"{phrase_replacement_dict['instruction']}\\n{phrase_replacement_dict['question']}: {question_data['question']}\\n{phrase_replacement_dict['choices']}: {choices} \\n{phrase_replacement_dict['answer']}:\"\n",
        "        return formatted\n",
        "\n",
        "    @abstractmethod\n",
        "    def answer_mcq(self, question_data: Dict, phrase_replacement_dict) -> Dict:\n",
        "        \"\"\"Abstract method for answering a multiple choice question\"\"\"\n",
        "        pass\n",
        "\n",
        "    def evaluate_task(self, task_data: List[Dict], style, output_dir: str = \"rephrased_results\", save_results: bool = True, num_samples: int = None) -> Dict:\n",
        "        \"\"\"Evaluate the multiple-choice task and return detailed metrics.\"\"\"\n",
        "\n",
        "        # If num_samples is provided, randomly sample `num_samples` questions\n",
        "        all_questions = task_data\n",
        "        if num_samples:\n",
        "            all_questions = random.sample(task_data, min(num_samples, len(task_data)))\n",
        "\n",
        "        metrics = {\n",
        "            \"total_questions\": len(all_questions),\n",
        "            \"correct\": 0,\n",
        "            \"wrong\": 0,\n",
        "            \"per_choice_stats\": defaultdict(lambda: {\"correct\": 0, \"total\": 0}),\n",
        "            \"confidence_when_correct\": [],\n",
        "            \"confidence_when_wrong\": [],\n",
        "            \"all_results\": [],\n",
        "            \"model_name\": self.model_name\n",
        "        }\n",
        "\n",
        "        # Wrap the question processing with tqdm for progress indication\n",
        "        for q in tqdm(all_questions, desc=\"Evaluating\", unit=\"question\"):\n",
        "            result = self.answer_mcq(q)\n",
        "            metrics[\"all_results\"].append(result)\n",
        "\n",
        "            correct = result[\"answer\"] == result[\"correct_answer\"]\n",
        "            if correct:\n",
        "                metrics[\"correct\"] += 1\n",
        "                metrics[\"confidence_when_correct\"].append(result[\"confidence\"])\n",
        "            else:\n",
        "                metrics[\"wrong\"] += 1\n",
        "                metrics[\"confidence_when_wrong\"].append(result[\"confidence\"])\n",
        "\n",
        "            # Per-choice stats\n",
        "            true = result[\"correct_answer\"]\n",
        "            if true:\n",
        "                metrics[\"per_choice_stats\"][true][\"total\"] += 1\n",
        "                if correct:\n",
        "                    metrics[\"per_choice_stats\"][true][\"correct\"] += 1\n",
        "\n",
        "        metrics[\"accuracy\"] = metrics[\"correct\"] / metrics[\"total_questions\"]\n",
        "        metrics[\"per_choice_accuracy\"] = {\n",
        "            k: {\"accuracy\": v[\"correct\"] / v[\"total\"], \"total\": v[\"total\"]} for k, v in metrics[\"per_choice_stats\"].items()\n",
        "        }\n",
        "\n",
        "        # Save results\n",
        "        if save_results:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            fname = f\"results_task_{self.model_name}_{style}_as_plain_{timestamp}.json\"\n",
        "            fname = fname.replace(\"/\", \"_\")\n",
        "            path = f\"{output_dir}/{fname}\"\n",
        "\n",
        "            # Convert defaultdict to regular dict for JSON\n",
        "            metrics[\"per_choice_stats\"] = dict(metrics[\"per_choice_stats\"])\n",
        "\n",
        "            with open(path, \"w\") as f:\n",
        "                json.dump(metrics, f, indent=2)\n",
        "\n",
        "        return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k95VdFvklU-L"
      },
      "source": [
        "# Llama 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbUfwajyoqdw"
      },
      "source": [
        "### install bits and bytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2t9eY1HGonic"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X21jXkXQovKa"
      },
      "source": [
        "### llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVCSj-mjDXIN"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "from typing import Dict\n",
        "\n",
        "class Llama2Model(MCQ):\n",
        "    def __init__(self, model_name: str = \"meta-llama/Llama-2-7b-hf\"):\n",
        "        super().__init__(model_name=model_name)\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "\n",
        "        # Initialize tokenizer and model\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\"\n",
        "        ).to(self.device)  # Move the model to the selected device\n",
        "\n",
        "    def answer_mcq(self, question_data: Dict) -> Dict:\n",
        "        \"\"\"Answer a single multiple-choice question using Llama Model\"\"\"\n",
        "        choices = question_data[\"answers\"]  # Now we use 'answers' as the choices dictionary\n",
        "        premise = question_data[\"question\"]\n",
        "        # Format the prompt with choices\n",
        "        prompt = question_data[\"plain\"]\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)  # Ensure inputs are on the correct device\n",
        "        outputs = self.model(**inputs)\n",
        "\n",
        "        # Get logits for the last token in the sequence\n",
        "        logits = outputs.logits[:, -1, :]  # Get logits for the last token\n",
        "        probabilities = torch.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
        "\n",
        "        # Map each choice to its token ID and calculate probabilities\n",
        "        choice_probabilities = {}\n",
        "        for choice, answer in choices.items():\n",
        "            # Encode the choice text to get its token ID\n",
        "            choice_token_id = self.tokenizer.encode(answer, add_special_tokens=False)[0]  # Token ID for the choice text\n",
        "            # Get the probability of the choice token in the model's output\n",
        "            choice_probabilities[choice] = probabilities[0, choice_token_id].item()\n",
        "\n",
        "        # Select the choice with the highest probability\n",
        "        predicted_choice = max(choice_probabilities, key=choice_probabilities.get)\n",
        "        confidence = choice_probabilities[predicted_choice]\n",
        "\n",
        "        return {\n",
        "            \"question\": premise,\n",
        "            \"answers\": choices,\n",
        "            \"answer\": predicted_choice,\n",
        "            \"confidence\": confidence,\n",
        "            \"correct_answer\": question_data[\"correct_answer\"],  # No need to access the first character anymore\n",
        "            \"prompt_used\": prompt\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJqxutD8lZVj"
      },
      "outputs": [],
      "source": [
        "llama2_model = Llama2Model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qDK-tJCHEDg"
      },
      "outputs": [],
      "source": [
        "evaluate_model(llama2_model, \"meta-llama_Llama-2-7b-hf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIr6m7noDPAL"
      },
      "source": [
        "# Llama 3.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPWChRoZDOOD"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "class Llama3BModel(MCQ):\n",
        "    def __init__(self, model_name: str = \"meta-llama/Llama-3.2-3B-Instruct\"):\n",
        "        super().__init__(model_name=model_name)\n",
        "\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.tokenizer.padding_side = 'left'\n",
        "\n",
        "        # Load the Llama model\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            torch_dtype=torch.bfloat16,  # Use bfloat16 for efficiency\n",
        "            device_map=\"auto\",           # Automatically map model layers to available devices\n",
        "            trust_remote_code=True       # Trust custom code if available in the model repo\n",
        "        ).to(self.device)  # Move the model to the correct device\n",
        "\n",
        "        # Set up the text generation pipeline\n",
        "        self.llm = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            pad_token_id=self.tokenizer.eos_token_id,\n",
        "            max_new_tokens=1,  # Generate a single token for efficiency\n",
        "            do_sample=False     # For deterministic output\n",
        "        )\n",
        "\n",
        "    def answer_mcq(self, question_data: Dict) -> Dict:\n",
        "        \"\"\"Answer a single multiple-choice question using Llama Model\"\"\"\n",
        "        choices = question_data[\"choices\"]  # Now we use 'answers' as the choices dictionary\n",
        "\n",
        "        # Format the prompt with choices\n",
        "        prompt = question_data[\"plain\"]\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)  # Ensure inputs are on the correct device\n",
        "\n",
        "        # Get model outputs (logits)\n",
        "        outputs = self.model(**inputs)\n",
        "        logits = outputs.logits[:, -1, :]  # Get logits for the last token in the sequence\n",
        "\n",
        "        # Apply softmax to get probabilities for each choice\n",
        "        probabilities = torch.softmax(logits, dim=-1)\n",
        "\n",
        "        # Map each choice to its token ID and calculate probabilities\n",
        "        choice_probabilities = {}\n",
        "        for idx, choice in enumerate(choices):\n",
        "            choice_token_id = self.tokenizer.encode(choice, add_special_tokens=False)[0]  # Token ID for the choice text\n",
        "            choice_probabilities[choice] = probabilities[0, choice_token_id].item()\n",
        "\n",
        "        # Select the choice with the highest probability\n",
        "        predicted_choice = max(choice_probabilities, key=choice_probabilities.get)\n",
        "        confidence = choice_probabilities[predicted_choice]\n",
        "\n",
        "        return {\n",
        "            \"question\": question_data[\"question\"],\n",
        "            \"choices\": choices,\n",
        "            \"answer\": predicted_choice,\n",
        "            \"confidence\": confidence,\n",
        "            \"correct_answer\": question_data[\"correct_answer\"],  # Assuming output field contains the correct answer\n",
        "            \"prompt_used\": prompt\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qs8kqHfzD5mk"
      },
      "outputs": [],
      "source": [
        "llama_3 = Llama3BModel()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(llama_3, \"meta-llama_Llama-3.2-3B-Instruct\")"
      ],
      "metadata": {
        "id": "2yDzRbe4rzeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2GkSNAAk-pO"
      },
      "source": [
        "# Bart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGOYk1HLDc_X"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "from transformers.models.bart.modeling_bart import BartForSequenceClassification # Explicitly import the model class\n",
        "\n",
        "\n",
        "# Define ENTAILMENT_IDX based on the task (usually 2 for MNLI)\n",
        "ENTAILMENT_IDX = 2\n",
        "\n",
        "class BartModel(MCQ):\n",
        "    def __init__(self, model_name: str = \"facebook/bart-large-mnli\"):\n",
        "        super().__init__(model_name=model_name)\n",
        "        # Load the tokenizer and model for BART from the 'facebook/bart-large-mnli' checkpoint\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def answer_mcq(self, question_data: dict, phrase_replacement_dict: dict) -> dict:\n",
        "        \"\"\"\n",
        "        Answer a single multiple-choice question efficiently using a BART-style model.\n",
        "        Choices are given as a dictionary (e.g., {\"A\": \"Choice1\", \"B\": \"Choice2\", ...})\n",
        "        \"\"\"\n",
        "\n",
        "        premise = question_data[\"question\"]\n",
        "        choices_dict = question_data[\"answers\"]  # e.g., {\"A\": \"Louis XIII\", ...}\n",
        "        choice_keys = list(choices_dict.keys())\n",
        "        choice_texts = list(choices_dict.values())\n",
        "\n",
        "        # Format the prompt with the phrase replacement dictionary\n",
        "        prompt = question_data[\"plain\"]\n",
        "\n",
        "        # Prepare the batched input: one sequence per choice\n",
        "        # Each input is: \"Prompt + Question + 'The answer is {choice_text}.'\"\n",
        "        batch_texts = [f\"{prompt}\\n\\nThe answer is {text}.\" for text in choice_texts]\n",
        "\n",
        "        # Tokenize as a batch\n",
        "        inputs = self.tokenizer(\n",
        "            batch_texts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True\n",
        "        ).to(self.model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(**inputs).logits\n",
        "            # Softmax over the last dimension and pick entailment index\n",
        "            probs = torch.softmax(logits, dim=1)[:, ENTAILMENT_IDX]\n",
        "\n",
        "        probs = probs.cpu().tolist()\n",
        "\n",
        "        # Pick the choice with the highest probability\n",
        "        max_idx = int(torch.tensor(probs).argmax())\n",
        "        predicted_key = choice_keys[max_idx]\n",
        "        predicted_answer = choice_texts[max_idx]\n",
        "        confidence = probs[max_idx]\n",
        "\n",
        "        return {\n",
        "            \"question\": premise,\n",
        "            \"choices\": choices_dict,\n",
        "            \"answer\": predicted_key,\n",
        "            \"confidence\": confidence,\n",
        "            \"correct_answer\": question_data.get(\"correct_answer\", None),\n",
        "            \"prompt_used\": prompt\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_KYYPAmGIHB"
      },
      "outputs": [],
      "source": [
        "bart = BartModel()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(bart, \"facebook_bart-large-mnli\")"
      ],
      "metadata": {
        "id": "tGesmKv7r_K1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deberta"
      ],
      "metadata": {
        "id": "bALgotNQoENv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PPSVG4iNbst"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, Any, List, Optional\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "class DeBERTaNLI_MCQ(MCQ):\n",
        "    \"\"\"\n",
        "    Multiple-choice via zero-shot NLI with:\n",
        "      model = MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\n",
        "      premise  = format_prompt(question_data, phrase_replacement_dict)\n",
        "      hypothesis(choice) = \"The answer is {choice}.\"\n",
        "      score = P(entailment | premise, hypothesis)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str =\n",
        "                 \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\"):\n",
        "        super().__init__(model_name=model_name)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name).to(self.device)\n",
        "        self.model.eval()\n",
        "        self.entail_idx = self._entailment_index()\n",
        "\n",
        "    def _entailment_index(self) -> int:\n",
        "        # Robustly detect the entailment label index from the model config\n",
        "        id2label = getattr(self.model.config, \"id2label\", None)\n",
        "        if isinstance(id2label, dict):\n",
        "            for i, name in id2label.items():\n",
        "                if str(name).lower().startswith(\"entail\"):\n",
        "                    return int(i)\n",
        "        # Fallback to common MNLI ordering (C, N, E) -> index 2\n",
        "        return 2\n",
        "\n",
        "    def answer_mcq(self, question_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        # Build premise with YOUR formatter (required as per your request)\n",
        "        premise = question_data[\"plain\"]\n",
        "\n",
        "        choices: Dict[str, str] = question_data[\"choices\"]  # {\"A\": \"...\", \"B\": \"...\"}\n",
        "        keys: List[str] = list(choices.keys())\n",
        "        texts: List[str] = [choices[k] for k in keys]\n",
        "\n",
        "        premises = [premise] * len(texts)\n",
        "        hyps = [f\"The answer is {t}.\" for t in texts]\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            premises,\n",
        "            hyps,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True\n",
        "        ).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(**inputs).logits  # [batch, 3]\n",
        "            probs = F.softmax(logits, dim=1)[:, self.entail_idx]  # entailment probs\n",
        "\n",
        "        best_idx = int(probs.argmax().item())\n",
        "        confidences = probs.detach().cpu().tolist()\n",
        "\n",
        "        return {\n",
        "            \"question\": question_data[\"question\"],\n",
        "            \"choices\": choices,\n",
        "            \"answer\": keys[best_idx],\n",
        "            \"answer_text\": texts[best_idx],\n",
        "            \"confidence\": confidences[best_idx],\n",
        "            \"all_entailment_probs\": {k: p for k, p in zip(keys, confidences)},\n",
        "            \"correct_answer\": question_data.get(\"correct_answer\"),\n",
        "            \"prompt_used\": premise,\n",
        "            \"model_name\": self.model.config._name_or_path\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deberta = DeBERTaNLI_MCQ()"
      ],
      "metadata": {
        "id": "GMFOIamvpHvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(deberta, \"MoritzLaurer_DeBERTa-v3-large-mnli-fever-anli-ling-wanli\")"
      ],
      "metadata": {
        "id": "YzMme6oCoDT4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "uXIxtVOiltaS",
        "k95VdFvklU-L",
        "LbUfwajyoqdw"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}